Challenging Problem
Building a small language model from scratch involves understanding every component that powers modern NLP systems. 
The challenge lies in designing custom tokenization, creating an efficient model architecture, managing long-range dependencies in text, and training the model on limited data without overfitting. 
It also requires balancing computational constraints while still achieving coherent text generation. This project tackles these difficulties step-by-step to show how a functional language model can be built at a small scale.
